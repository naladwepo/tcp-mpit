"""
LLM-based –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
"""

import json
import re
from typing import List, Dict, Optional
from pathlib import Path


class LLMQueryPreprocessor:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    """
    
    def __init__(self, model_path: str = "./Qwen/Qwen3-4B-Instruct-2507", use_llm: bool = True):
        """
        Args:
            model_path: –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ Qwen
            use_llm: –ø—ã—Ç–∞—Ç—å—Å—è –ª–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å LLM (False = —Å—Ä–∞–∑—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä)
        """
        self.model_path = Path(model_path)
        self.model = None
        self.tokenizer = None
        self.use_llm = use_llm
        
        if use_llm:
            self._load_model()
        else:
            print("üìù LLM –æ—Ç–∫–ª—é—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä")
    
    def _load_model(self):
        """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏
        if not self.model_path.exists():
            print(f"‚ö†Ô∏è –ü—É—Ç—å –∫ LLM –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.model_path}")
            print("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –±–µ–∑ LLM")
            self.model = None
            return
        
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}...")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    str(self.model_path),
                    trust_remote_code=True
                )
                print("‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {e}")
                raise
            
            # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    str(self.model_path),
                    device_map="cpu",
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                print("‚úì LLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                raise
            
        except ImportError as e:
            print(f"‚ö†Ô∏è –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏: {e}")
            print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch")
            print("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –±–µ–∑ LLM")
            self.model = None
            self.tokenizer = None
            
        except Exception as e:
            error_msg = str(e)
            if "ModelWrapper" in error_msg:
                print(f"‚ö†Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –≤–µ—Ä—Å–∏—è transformers –¥–ª—è –º–æ–¥–µ–ª–∏ Qwen3")
                print("Qwen3 —Ç—Ä–µ–±—É–µ—Ç transformers >= 4.51.0")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –≤–µ—Ä—Å–∏—é
                try:
                    import transformers
                    current_version = transformers.__version__
                    print(f"–¢–µ–∫—É—â–∞—è –≤–µ—Ä—Å–∏—è: {current_version}")
                except:
                    pass
                
                print("–û–±–Ω–æ–≤–∏—Ç–µ: pip install --upgrade transformers")
            elif "safetensors" in error_msg.lower():
                print(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å —Ñ–∞–π–ª–∞–º–∏ –º–æ–¥–µ–ª–∏ (safetensors)")
            else:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å LLM: {error_msg[:100]}")
            
            print("–ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –±–µ–∑ LLM")
            self.model = None
            self.tokenizer = None
    
    def decompose_query(self, query: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è LLM
        
        Args:
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            List[str]: —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        # –ï—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä
        if self.model is None or self.tokenizer is None:
            return self._simple_decompose(query)
        
        try:
            # –ü—Ä–æ–º–ø—Ç –¥–ª—è LLM
            prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –ø–æ–∏—Å–∫ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.

–ó–∞–¥–∞—á–∞: —Ä–∞–∑–±–µ–π —Å–ª–æ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞.

–ü—Ä–∞–≤–∏–ª–∞:
1. –ò–∑–≤–ª–µ–∫–∏ –∫–∞–∂–¥—ã–π –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–æ–≤–∞—Ä –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
2. –°–æ—Ö—Ä–∞–Ω–∏ –≤–∞–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ä–∞–∑–º–µ—Ä—ã, –º–∞—Ç–µ—Ä–∏–∞–ª)
3. –î–ª—è –∫—Ä–µ–ø–µ–∂–∞ (–≤–∏–Ω—Ç—ã, –≥–∞–π–∫–∏) –∏—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è (–ú6, –ú8, –ú10)
4. –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –≤–∏–¥–µ JSON —Å–ø–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫

–ü—Ä–∏–º–µ—Ä 1:
–ó–∞–ø—Ä–æ—Å: "–ö–æ–º–ø–ª–µ–∫—Ç –¥–ª—è –º–æ–Ω—Ç–∞–∂–∞ –∫–æ—Ä–æ–±–∞ 200x200: –∫–æ—Ä–æ–±, –∫—Ä—ã—à–∫–∞, –≤–∏–Ω—Ç—ã –∏ –≥–∞–π–∫–∏"
–†–µ–∑—É–ª—å—Ç–∞—Ç: ["–ö–æ—Ä–æ–± 200x200", "–ö—Ä—ã—à–∫–∞ 200", "–í–∏–Ω—Ç –ú6", "–ì–∞–π–∫–∞ –ú6"]

–ü—Ä–∏–º–µ—Ä 2:
–ó–∞–ø—Ä–æ—Å: "–õ–æ—Ç–æ–∫ –ø–µ—Ä—Ñ–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π 600 –º–º"
–†–µ–∑—É–ª—å—Ç–∞—Ç: ["–õ–æ—Ç–æ–∫ –ø–µ—Ä—Ñ–æ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π 600 –º–º"]

–ü—Ä–∏–º–µ—Ä 3:
–ó–∞–ø—Ä–æ—Å: "–ì–∞–π–∫–∞ –ú6"
–†–µ–∑—É–ª—å—Ç–∞—Ç: ["–ì–∞–π–∫–∞ –ú6"]

–ó–∞–ø—Ä–æ—Å: "{query}"
–†–µ–∑—É–ª—å—Ç–∞—Ç:"""

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            messages = [
                {"role": "system", "content": "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã."},
                {"role": "user", "content": prompt}
            ]
            
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=256,
                temperature=0.1,
                do_sample=False
            )
            
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            components = self._parse_llm_response(response)
            
            if components:
                print(f"‚úì LLM —Ä–∞–∑–±–∏–ª –∑–∞–ø—Ä–æ—Å –Ω–∞ {len(components)} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞(–æ–≤)")
                for i, comp in enumerate(components, 1):
                    print(f"  {i}. {comp}")
                return components
            else:
                # Fallback –µ—Å–ª–∏ LLM –Ω–µ —Å–º–æ–≥ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å
                print("‚ö†Ô∏è LLM –Ω–µ —Å–º–æ–≥ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä")
                return self._simple_decompose(query)
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ LLM –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏: {e}")
            print("–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä...")
            return self._simple_decompose(query)
    
    def _parse_llm_response(self, response: str) -> Optional[List[str]]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM"""
        try:
            # –ò—â–µ–º JSON –º–∞—Å—Å–∏–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            json_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if json_match:
                components = json.loads(json_match.group(0))
                if isinstance(components, list) and all(isinstance(c, str) for c in components):
                    return [c.strip() for c in components if c.strip()]
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
            lines = response.strip().split('\n')
            components = []
            for line in lines:
                line = line.strip()
                # –£–±–∏—Ä–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã —Å–ø–∏—Å–∫–∞
                line = re.sub(r'^[-*‚Ä¢]\s*', '', line)
                line = re.sub(r'^\d+\.\s*', '', line)
                # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏
                line = line.strip('"\'')
                if line and len(line) > 2:
                    components.append(line)
            
            if components:
                return components
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ LLM: {e}")
        
        return None
    
    def _simple_decompose(self, query: str) -> List[str]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä –±–µ–∑ LLM (fallback)
        
        Args:
            query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            List[str]: —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
        if ':' not in query and ',' not in query:
            return [query]
        
        print("üìù –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–µ—Ä (–±–µ–∑ LLM)")
        components = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–º–µ—Ä –µ—Å–ª–∏ –µ—Å—Ç—å
        size_match = re.search(r'(\d+x\d+)', query, re.IGNORECASE)
        size = size_match.group(1) if size_match else None
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –¥–≤–æ–µ—Ç–æ—á–∏—é
        if ':' in query:
            _, after_colon = query.split(':', 1)
            parts = after_colon.split(',')
        else:
            parts = query.split(',')
        
        for part in parts:
            part = part.strip()
            if not part or len(part) < 2:
                continue
            
            # –î–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä
            if size and any(word in part.lower() for word in ['–∫–æ—Ä–æ–±', '–ª–æ—Ç–æ–∫']):
                if size not in part:
                    part = f"{part} {size}"
            
            # –î–ª—è –∫—Ä—ã—à–µ–∫ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä 200x200, —Ç–æ–ª—å–∫–æ —à–∏—Ä–∏–Ω—É
            if size and '–∫—Ä—ã—à–∫–∞' in part.lower():
                if size not in part:
                    width = size.split('x')[0]  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —à–∏—Ä–∏–Ω—É (200)
                    part = f"{part} {width}"
            
            # –î–ª—è –∫—Ä–µ–ø–µ–∂–∞ –∑–∞–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ
            if any(word in part.lower() for word in ['–≤–∏–Ω—Ç', '–≥–∞–π–∫–∞', '–±–æ–ª—Ç']):
                # –£–±–∏—Ä–∞–µ–º –±–æ–ª—å—à–∏–µ —Ä–∞–∑–º–µ—Ä—ã
                part = re.sub(r'\d+x\d+', '', part).strip()
                # –î–æ–±–∞–≤–ª—è–µ–º –ú6 –µ—Å–ª–∏ –Ω–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è
                if not re.search(r'–ú\d+', part, re.IGNORECASE):
                    part = f"{part} –ú6"
            
            components.append(part)
        
        return components if components else [query]
