"""
LLM-–ø–∞—Ä—Å–µ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –≤—ã–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
"""

import json
import re
from typing import List, Dict, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


class LLMRequestParser:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    –í—ã–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON:
    {
        "items": [
            {"name": "–ö–æ—Ä–æ–± 200x200", "quantity": 1, "specifications": "200x200 –º–º", "top_k": 3},
            {"name": "–ö—Ä—ã—à–∫–∞", "quantity": 1, "specifications": "200 –º–º", "top_k": 3},
            {"name": "–í–∏–Ω—Ç –ú6", "quantity": 4, "specifications": "–ú6", "top_k": 5},
            {"name": "–ì–∞–π–∫–∞ –ú6", "quantity": 4, "specifications": "–ú6", "top_k": 5}
        ]
    }
    
    top_k - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ (1-10):
    - –î–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ (—Å —Ç–æ—á–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏): 2-3
    - –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ (–∫—Ä–µ–ø–µ–∂, –º–µ—Ç–∏–∑—ã): 5-7
    - –î–ª—è –æ–±—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: 3-5
    """
    
    def __init__(
        self,
        model_path: str = "Qwen/Qwen3-4B-Instruct-2507",
        device: Optional[str] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            model_path: –ø—É—Ç—å –∫ LLM –º–æ–¥–µ–ª–∏
            device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ('cuda', 'mps', 'cpu') –∏–ª–∏ None –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        """
        self.model_path = model_path
        self.device = self._detect_device(device)
        self.model = None
        self.tokenizer = None
        
        print(f"üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self._load_model()
    
    def _detect_device(self, device: Optional[str] = None) -> str:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: CUDA > MPS > CPU
        
        Args:
            device: —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏–ª–∏ None
            
        Returns:
            str: –Ω–∞–∑–≤–∞–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ('cuda', 'mps', 'cpu')
        """
        if device:
            return device
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA (NVIDIA GPU)
        if torch.cuda.is_available():
            print("‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ CUDA (NVIDIA GPU)")
            return "cuda"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º MPS (Apple Silicon GPU)
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            print("‚úì –û–±–Ω–∞—Ä—É–∂–µ–Ω MPS (Apple Silicon GPU)")
            return "mps"
        
        # Fallback –Ω–∞ CPU
        print("‚ÑπÔ∏è  GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        return "cpu"
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç LLM –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA/MPS/CPU"""
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ LLM –ø–∞—Ä—Å–µ—Ä–∞ –∏–∑ {self.model_path}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        print("‚úì –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º dtype –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if self.device == "cuda":
            torch_dtype = torch.float16
            device_map = "auto"
        elif self.device == "mps":
            torch_dtype = torch.float16
            device_map = None
        else:  # CPU
            torch_dtype = torch.float32
            device_map = None
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            device_map=device_map if self.device == "cuda" else None,
            torch_dtype=torch_dtype,
            trust_remote_code=True
        )
        
        # –Ø–≤–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map
        if self.device != "cuda":
            self.model = self.model.to(self.device)
        
        print(f"‚úì LLM –ø–∞—Ä—Å–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {self.device}")
    
    def parse_request(self, user_query: str) -> Dict:
        """
        –ü–∞—Ä—Å–∏—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤
        
        Args:
            user_query: –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏:
                - items: List[Dict] - —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
                - confidence: float - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–±–æ—Ä–µ (0-1)
                - analysis: str - –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        """
        prompt = self._build_prompt(user_query)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM
        response = self._generate(prompt)
        
        # –ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        parsed = self._parse_response(response)
        
        if parsed:
            return parsed
        else:
            # Fallback: —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥
            return self._heuristic_parse(user_query)
    
    def _build_prompt(self, user_query: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM"""
        prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–¥—É–∫—Ü–∏–∏.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: —Ä–∞–∑–æ–±—Ä–∞—Ç—å –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –≤—ã–¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º.

–ü–†–ê–í–ò–õ–ê:
1. –û–ø—Ä–µ–¥–µ–ª–∏ –í–°–ï —Ç–æ–≤–∞—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
2. –£–∫–∞–∂–∏ —Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
3. –û–ø—Ä–µ–¥–µ–ª–∏ top_k (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞, 1-10):
   - –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ —Å —Ç–æ—á–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è–º–∏: 2-3
   - –î–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∫—Ä–µ–ø–µ–∂–∞ (–≤–∏–Ω—Ç—ã, –≥–∞–π–∫–∏, —à–∞–π–±—ã): 5-7
   - –î–ª—è –æ–±—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –±–µ–∑ —Ç–æ—á–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: 3-5
4. –î–ª—è –∫–æ–º–ø–ª–µ–∫—Ç–æ–≤ –º–æ–Ω—Ç–∞–∂–∞ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã:
   - –ö–æ—Ä–æ–±/–ª–æ—Ç–æ–∫: 1 —à—Ç, top_k: 3
   - –ö—Ä—ã—à–∫–∞: 1 —à—Ç, top_k: 3
   - –í–∏–Ω—Ç—ã: 4 —à—Ç, top_k: 5 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –º–æ–Ω—Ç–∞–∂–∞)
   - –ì–∞–π–∫–∏: 4 —à—Ç, top_k: 5 (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã)
   - –®–∞–π–±—ã: 4 —à—Ç, top_k: 5 (–µ—Å–ª–∏ –Ω—É–∂–Ω—ã)
5. –°–æ—Ö—Ä–∞–Ω—è–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—Ä–∞–∑–º–µ—Ä—ã, —Ä–µ–∑—å–±—É, –º–∞—Ç–µ—Ä–∏–∞–ª)

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (—Ç–æ–ª—å–∫–æ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):
{{
    "items": [
        {{"name": "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞", "quantity": —á–∏—Å–ª–æ, "specifications": "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏", "top_k": 1-10}},
        ...
    ],
    "confidence": 0.0-1.0,
    "analysis": "–∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞"
}}

–ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
{user_query}

–û–¢–í–ï–¢ (—Ç–æ–ª—å–∫–æ JSON):"""
        
        return prompt
    
    def _generate(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
        messages = [
            {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤. –û—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON."},
            {"role": "user", "content": prompt}
        ]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        
        generated_ids = self.model.generate(
            **model_inputs,
            max_new_tokens=512,
            temperature=0.3,
            top_p=0.9,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id
        )
        
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response
    
    def _parse_response(self, response: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(0))
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                if 'items' in data and isinstance(data['items'], list):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–≤–∞—Ä
                    for item in data['items']:
                        if 'name' not in item or 'quantity' not in item:
                            return None
                    
                    return data
            
            return None
            
        except json.JSONDecodeError as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return None
    
    def _heuristic_parse(self, user_query: str) -> Dict:
        """
        –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ (fallback –µ—Å–ª–∏ LLM –Ω–µ —Å–ø—Ä–∞–≤–∏–ª–∞—Å—å)
        """
        query_lower = user_query.lower()
        items = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        is_assembly = any(kw in query_lower for kw in ['–∫–æ–º–ø–ª–µ–∫—Ç', '–º–æ–Ω—Ç–∞–∂', '—É—Å—Ç–∞–Ω–æ–≤–∫–∞', '–Ω–∞–±–æ—Ä'])
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–æ–≤–∞—Ä–æ–≤
        patterns = [
            (r'–∫–æ—Ä–æ–±\s*(\d+x\d+)?', '–ö–æ—Ä–æ–±', 1, 3),  # (pattern, name, default_qty, top_k)
            (r'–∫—Ä—ã—à–∫[–∞–∏]', '–ö—Ä—ã—à–∫–∞', 1, 3),
            (r'–≤–∏–Ω—Ç[—ã]?\s*(–ú\d+)?', '–í–∏–Ω—Ç', 4 if is_assembly else 1, 5),
            (r'–≥–∞–π–∫[–∞–∏]?\s*(–ú\d+)?', '–ì–∞–π–∫–∞', 4 if is_assembly else 1, 5),
            (r'—à–∞–π–±[–∞—ã]?\s*(–ú\d+)?', '–®–∞–π–±–∞', 4 if is_assembly else 1, 5),
            (r'–ª–æ—Ç–æ–∫\s*(\d+)?', '–õ–æ—Ç–æ–∫', 1, 3),
        ]
        
        for pattern, name, default_qty, default_top_k in patterns:
            match = re.search(pattern, query_lower, re.IGNORECASE)
            if match:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É–∫–∞–∑–∞–Ω–æ –ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–≤–Ω–æ
                qty_match = re.search(rf'(\d+)\s+{pattern}', query_lower)
                quantity = int(qty_match.group(1)) if qty_match else default_qty
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
                specs = match.group(1) if match.lastindex else ""
                
                items.append({
                    "name": f"{name} {specs}".strip(),
                    "quantity": quantity,
                    "specifications": specs,
                    "top_k": default_top_k
                })
        
        return {
            "items": items if items else [{"name": user_query, "quantity": 1, "specifications": "", "top_k": 3}],
            "confidence": 0.6,  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —ç–≤—Ä–∏—Å—Ç–∏–∫
            "analysis": f"–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –ø–æ–∑–∏—Ü–∏–π"
        }
    
    def format_result(self, parsed_result: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞"""
        lines = []
        lines.append("=" * 60)
        lines.append("üìã –ê–ù–ê–õ–ò–ó –ó–ê–ü–†–û–°–ê")
        lines.append("=" * 60)
        lines.append(f"\nüí° {parsed_result.get('analysis', 'N/A')}")
        lines.append(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {parsed_result.get('confidence', 0) * 100:.0f}%")
        lines.append(f"\nüì¶ –¢–û–í–ê–†–´ –ö –ü–û–ò–°–ö–£ ({len(parsed_result.get('items', []))} –ø–æ–∑.):")
        lines.append("-" * 60)
        
        for i, item in enumerate(parsed_result.get('items', []), 1):
            name = item.get('name', 'N/A')
            qty = item.get('quantity', 1)
            specs = item.get('specifications', '')
            top_k = item.get('top_k', 3)
            
            lines.append(f"\n{i}. {name}")
            lines.append(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {qty} —à—Ç.")
            if specs:
                lines.append(f"   –°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è: {specs}")
            lines.append(f"   –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞: {top_k}")
        
        lines.append("\n" + "=" * 60)
        return "\n".join(lines)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    parser = LLMRequestParser()
    
    test_queries = [
        "–ö–æ—Ä–æ–± 200x200",
        "–ö–æ–º–ø–ª–µ–∫—Ç –¥–ª—è –º–æ–Ω—Ç–∞–∂–∞ –∫–æ—Ä–æ–±–∞: –∫–æ—Ä–æ–± 200x200, –∫—Ä—ã—à–∫–∞, –≤–∏–Ω—Ç—ã –ú6 –∏ –≥–∞–π–∫–∏ –ú6",
        "–ù—É–∂–µ–Ω –ª–æ—Ç–æ–∫ 600 –º–º —Å –∫—Ä—ã—à–∫–æ–π",
        "5 –≥–∞–µ–∫ –ú8 –∏ 5 –≤–∏–Ω—Ç–æ–≤ –ú8"
    ]
    
    for query in test_queries:
        print(f"\n{'='*70}")
        print(f"–ó–ê–ü–†–û–°: {query}")
        print('='*70)
        
        result = parser.parse_request(query)
        print(parser.format_result(result))
